{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojohnkim/mmai869/blob/main/2024_869_JohnKim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MMAI 869 2024: Individual Assignment\n",
        "\n",
        "- Student Name: Kim, John\n",
        "- Student Number: 20439250\n",
        "- Section Number: MMAI 2024\n",
        "- Favourite Book: Pachinko by Min Jin Lee\n",
        "- Currently Reading: The Worlds I See by Fei Fei Li\n",
        "- Submitted Date: 2024-01-07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj34Jz-Do_oK"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQ_XOKyXTS6",
        "outputId": "25fe5c2c-939f-49eb-aaf6-0f3caffc998c"
      },
      "outputs": [],
      "source": [
        "print(datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfOMt1lErLhZ",
        "outputId": "a667defb-07cb-493a-94b1-5d906420cc07"
      },
      "outputs": [],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aub2w1-arM5K",
        "outputId": "27c8b598-d2cc-4e89-b5d0-204d57ead224"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Y_n_8UrO9i",
        "outputId": "4a5563d9-1834-429b-ce97-fbd49a00ea5f"
      },
      "outputs": [],
      "source": [
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLlBjIyS2o54"
      },
      "source": [
        "# Question 1: Uncle Steve's Diamonds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yww0-vtpOw7z"
      },
      "source": [
        "## 1.0: Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import the packages\n",
        "import sklearn\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVWx2c-DhQYo",
        "outputId": "c63c2c7d-17e8-437c-a81e-543d28a45c0a"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "df1 = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1thHDCwQK3GijytoSSZNekAsItN_FGHtm\")\n",
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "zGojixXVCVNY",
        "outputId": "743471ec-66a2-4c2c-be36-f9ca5705d330"
      },
      "outputs": [],
      "source": [
        "## Data Exploration\n",
        "df1.describe().transpose()\n",
        "list(df1)\n",
        "df1.shape\n",
        "df1.info()\n",
        "df1.head(n=20)\n",
        "df1.describe().transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-processing steps before clustering \n",
        "# use standard scaler to standardize the values in the data before we apply clsutering\n",
        "scaler = StandardScaler()\n",
        "features = df1.columns\n",
        "X[features] = scaler.fit_transform(X[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape\n",
        "X.info()\n",
        "X.describe().transpose()\n",
        "X.head(10)\n",
        "X.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure();\n",
        "\n",
        "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=\"black\");\n",
        "plt.title(\"Jewelry Customers Data\");\n",
        "plt.xlabel('Age');\n",
        "plt.ylabel('Income');\n",
        "plt.xticks();\n",
        "plt.yticks();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure();\n",
        "\n",
        "plt.scatter(df1.iloc[:, 2], df1.iloc[:, 3], c=\"black\");\n",
        "plt.title(\"Jewelry Customer Data\");\n",
        "plt.xlabel('Spending Score');\n",
        "plt.ylabel('Savings');\n",
        "plt.xticks();\n",
        "plt.yticks();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure();\n",
        "\n",
        "plt.scatter(df1.iloc[:, 1], df1.iloc[:, 2], c=\"black\");\n",
        "plt.title(\"Jewelry Customer Data\");\n",
        "plt.xlabel('Income');\n",
        "plt.ylabel('Spending Score');\n",
        "plt.xticks();\n",
        "plt.yticks();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R04NzckZKbG2"
      },
      "source": [
        "## 1.1: Clustering Algorithm #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Using Elbow method to fid the number of clusters (k)\n",
        "\n",
        "inertias = {}\n",
        "silhouettes = {}\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init= 10).fit(X)\n",
        "    inertias[k] = kmeans.inertia_  \n",
        "    silhouettes[k] = silhouette_score(X, kmeans.labels_, metric='euclidean')\n",
        "\n",
        "plt.figure();\n",
        "plt.plot(list(inertias.keys()), list(inertias.values()));\n",
        "plt.title('K-Means, Elbow Method')\n",
        "plt.xlabel(\"Number of clusters, K\");\n",
        "plt.ylabel(\"Inertia\");\n",
        "\n",
        "\n",
        "plt.figure();\n",
        "plt.plot(list(silhouettes.keys()), list(silhouettes.values()));\n",
        "plt.title('K-Means, Elbow Method')\n",
        "plt.xlabel(\"Number of clusters, K\");\n",
        "plt.ylabel(\"Silhouette\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on the above Elbow Method Result, we will use a K range between 3 to 5\n",
        "\n",
        "def do_kmeans(df1, k):\n",
        "    k_means = KMeans(init='k-means++', n_clusters=k, n_init=10, random_state=42)\n",
        "    k_means.fit(df1)\n",
        "\n",
        "    plt.figure();\n",
        "    plt.scatter(df1.iloc[:, 1], df1.iloc[:, 2], c=k_means.labels_)\n",
        "    plt.scatter(k_means.cluster_centers_[:, 1], k_means.cluster_centers_[:, 2], marker='x', c=\"black\")\n",
        "    plt.title(\"K-Means (K={})\".format(k));\n",
        "    plt.xlabel('Income (K)');\n",
        "    plt.ylabel('Spending Score');\n",
        "    #plt.savefig('out/simple_kmeans_k{}.png'.format(k))\n",
        "    plt.show()\n",
        "\n",
        "    wcss = k_means.inertia_\n",
        "    sil = silhouette_score(df1, k_means.labels_)\n",
        "    print(\"K={}, WCSS={:.2f}, Sil={:.2f}\".format(k, wcss, sil))\n",
        "\n",
        "for k in range(3, 6):\n",
        "    do_kmeans(df1, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihVtYBWg1NM6"
      },
      "source": [
        "## 1.2: Clustering Algorithm #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\n",
        "agg.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "silhouette_score(X, agg.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure();\n",
        "\n",
        "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=agg.labels_);\n",
        "plt.title(\"Agglomerative\");\n",
        "plt.xlabel('Annual Income (K)');\n",
        "plt.ylabel('Spending Score');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.cluster\n",
        "\n",
        "aggl = scipy.cluster.hierarchy.linkage(X, method='ward', metric='euclidean')\n",
        "\n",
        "# Plot the dendogram\n",
        "plt.figure(figsize=(16, 8));\n",
        "plt.grid(False)\n",
        "plt.title(\"USD Dendogram\");  \n",
        "dend = scipy.cluster.hierarchy.dendrogram(aggl); "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ySJIgNr1Sfy"
      },
      "source": [
        "## 1.3 Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "K-Means Clustering had a silhouette score of 0.68, which is also indicative of a reasonably good cluster structure.\n",
        "Hierarchical Clustering has a higher silhouette score of approximately 0.805, suggesting an even better cluster structure compared to K-means in this specific instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2EAnCJ1Xta"
      },
      "source": [
        "## 1.4 Personas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_labels = agg.fit_predict(X)\n",
        "\n",
        "# Add the cluster labels as a new column to your DataFrame\n",
        "df1['ClusterID'] = cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the aggregation dictionary\n",
        "agg_funcs = {\n",
        "    'Age': ['min', 'mean', 'max'],\n",
        "    'Income': ['min', 'mean', 'max', 'var'],\n",
        "    'SpendingScore': ['min', 'mean', 'max', 'var'],\n",
        "    'Savings': ['min', 'mean', 'max', 'var']\n",
        "}\n",
        "\n",
        "# Apply the aggregation functions to each cluster\n",
        "cluster_stats = df1.groupby('ClusterID').agg(agg_funcs)\n",
        "\n",
        "# Flatten the MultiIndex columns\n",
        "cluster_stats.columns = ['_'.join(col) if type(col) is tuple else col for col in cluster_stats.columns.values]\n",
        "\n",
        "# Reset the index to make 'ClusterID' a column\n",
        "cluster_stats = cluster_stats.reset_index()\n",
        "\n",
        "# Calculate the number of instances per cluster and convert it to a DataFrame for merging\n",
        "instances_df = df1['ClusterID'].value_counts().sort_index().reset_index()\n",
        "instances_df.columns = ['ClusterID', 'NumberOfInstances']\n",
        "\n",
        "# Merge the number of instances with the cluster stats DataFrame\n",
        "cluster_stats = cluster_stats.merge(instances_df, on='ClusterID')\n",
        "\n",
        "# Display the cluster statistics with the number of instances included\n",
        "print(cluster_stats.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating customer personas involves interpreting the statistical data to build a semi-fictional character that embodies the traits of a particular customer segment. Here's how one might define the personas for each cluster of Diamond Store customers based on the provided statistics:\n",
        "\n",
        "### Cluster 0: \"Retired Savers\"\n",
        "- **Age Range**: Seniors (79-97 years old)\n",
        "- **Income Level**: Lower income (12,000 - 46,977)\n",
        "- **Spending Score**: Very low (0.20 - 0.47)\n",
        "- **Savings**: High (13,470 - 20,000)\n",
        "- **Size**: Largest segment (147 instances)\n",
        "- Persona: This cluster represents retired individuals who are likely on a fixed income. They are conservative spenders with a significant amount of savings, indicating a tendency to save rather than spend.\n",
        "\n",
        "### Cluster 1: \"Middle-Aged Professionals\"\n",
        "- **Age Range**: Middle-aged (51-68 years old)\n",
        "- **Income Level**: Middle to upper-middle income (56,321 - 90,422)\n",
        "- **Spending Score**: High (0.65 - 0.91)\n",
        "- **Savings**: Moderate (4,077 - 10,547)\n",
        "- **Size**: Second largest segment (157 instances)\n",
        "- Persona: This group consists of established professionals who are enjoying the fruits of their labor. They have a high spending score, indicating a willingness to make significant purchases, balanced with a sensible amount of savings.\n",
        "\n",
        "### Cluster 2: \"Young Affluents\"\n",
        "- **Age Range**: Young adults (22-44 years old)\n",
        "- **Income Level**: High income (89,598 - 119,877)\n",
        "- **Spending Score**: Moderate (0.17 - 0.41)\n",
        "- **Savings**: Moderate to high (12,207 - 17,968)\n",
        "- **Size**: Third largest segment (126 instances)\n",
        "- Persona: This segment is likely composed of young, affluent professionals who earn a lot but are moderate in their spending. They may be focused on building wealth and investing in their future.\n",
        "\n",
        "### Cluster 3: \"Wealthy Elite\"\n",
        "- **Age Range**: Young (17-31 years old)\n",
        "- **Income Level**: Very high income (117,108 - 142,000)\n",
        "- **Spending Score**: Extremely high (0.80 - 1.00)\n",
        "- **Savings**: Low (0 - 6,089)\n",
        "- **Size**: Smaller segment (50 instances)\n",
        "- Persona: The \"Wealthy Elite\" are likely young entrepreneurs or professionals with very high incomes and the highest spending scores, indicating a lifestyle of luxury and high-end purchases. Their savings are low, suggesting a preference for enjoying the present rather than saving for the future.\n",
        "\n",
        "### Cluster 4: \"Senior Elites\"\n",
        "- **Age Range**: Older seniors (77-93 years old)\n",
        "- **Income Level**: Very high (110,582 - 128,596)\n",
        "- **Spending Score**: Very low (0.00 - 0.15)\n",
        "- **Savings**: High (12,554 - 17,833)\n",
        "- **Size**: Smallest segment (25 instances)\n",
        "- Persona: This small, exclusive group consists of older individuals with very high incomes but very low spending scores, which could be indicative of wealth accumulated over a lifetime. They have high savings, suggesting a frugal lifestyle despite their wealth.\n",
        "\n",
        "These personas are a starting point and would benefit from qualitative data to add depth, such as lifestyle preferences or buying motivations. This additional information can turn statistical data into a more complete and actionable customer profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYwuYIgczYSv"
      },
      "source": [
        "# Question 2: Uncle Steve's Fine Foods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 High Support & High Confidence\n",
        "\n",
        "- **Rule**: `{organic kale} -> {quinoa}`  \n",
        "- The pairing of organic kale and quinoa is a staple in many health-conscious diets and recipes. For Uncle Steve, this combination is likely a common occurrence, but it provides an opportunity for targeted marketing. He could enhance this by creating health-focused displays, combining these items with other superfoods. Furthermore, hosting health and wellness events or cooking demos featuring these ingredients could elevate the customer experience and position the store as a destination for healthy living.\n",
        "\n",
        "### 2.2 High Support & Low Confidence\n",
        "\n",
        "- **Rule**: `{sourdough bread} -> {organic olive oil}`  \n",
        "- Sourdough or artisinal bread and organic olive oil may not always be purchased together frequently altough they are commonly paired together. This can perhaps be leveraged for cross-promotional strategies. Uncle Steve could introduce showcase these items together, possibly paired with tasting booths. This not only promotes the sale of both items but also enhances the customer's shopping experience by providing them with ideas for pairing and usage\n",
        "\n",
        "### 2.3 Low Support & Low Confidence\n",
        "\n",
        "- **Rule**: `{gluten-free pasta} -> {almond milk}`  \n",
        "- The purchase of gluten-free pasta along with almond milk might occur infrequently and reflects a specific dietary preference for vegans, indicating a segment focused on specific health or dietary needs. Uncle Steve can use this insight to curate a section dedicated to special dietary requirements, featuring gluten-free, dairy-free, and other health-conscious products. This not only serves the needs of a particular customer group but also establishes the store as an inclusive and considerate shopping destination. In addition, running health-focused promotions and collaborating with nutritionists for in-store events can further cement the store's reputation as a leader in catering to diverse dietary needs.\n",
        "\n",
        "### 2.4 Low Support & High Confidence\n",
        "\n",
        "- **Rule**: `{vegan cheese} -> {plant-based meat alternatives}`  \n",
        "-  The correlation between purchasing vegan cheese and plant-based meat alternatives is indicative of a growing trend towards veganism. While the market segment for these products is specific and not widespread, those who buy one are likely to buy the other. Recognizing this, Uncle Steve could create a dedicated 'vegan corner' in his store, grouping all plant-based alternatives together. This not only makes it easier for vegan customers to shop but also introduces non-vegan customers to these alternatives. Moreover, hosting vegan cooking classes or partnering with local vegan influencers for in-store promotions could increase the visibility and appeal of these products, potentially attracting a broader customer base interested in exploring plant-based diets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_IHoz7f2yIV"
      },
      "source": [
        "# Question 3: Uncle Steve's Credit Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqm_REd4oouz"
      },
      "source": [
        "## 3.0: Load data and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6b_BM0Nz9sF",
        "outputId": "0ea6da79-f51b-4f05-e2e1-f0eea595449c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6000 entries, 0 to 5999\n",
            "Data columns (total 17 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   UserID             6000 non-null   object\n",
            " 1   Sex                6000 non-null   object\n",
            " 2   PreviousDefault    6000 non-null   int64 \n",
            " 3   FirstName          6000 non-null   object\n",
            " 4   LastName           6000 non-null   object\n",
            " 5   NumberPets         6000 non-null   int64 \n",
            " 6   PreviousAccounts   6000 non-null   int64 \n",
            " 7   ResidenceDuration  6000 non-null   int64 \n",
            " 8   Street             6000 non-null   object\n",
            " 9   LicensePlate       6000 non-null   object\n",
            " 10  BadCredit          6000 non-null   int64 \n",
            " 11  Amount             6000 non-null   int64 \n",
            " 12  Married            6000 non-null   int64 \n",
            " 13  Duration           6000 non-null   int64 \n",
            " 14  City               6000 non-null   object\n",
            " 15  Purpose            6000 non-null   object\n",
            " 16  DateOfBirth        6000 non-null   object\n",
            "dtypes: int64(8), object(9)\n",
            "memory usage: 797.0+ KB\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# First, we'll read the provided labeled training data\n",
        "df3 = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1wOhyCnvGeY4jplxI8lZ-bbYN3zLtickf\")\n",
        "df3.info()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df3.drop('BadCredit', axis=1) #.select_dtypes(['number'])\n",
        "y = df3['BadCredit']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdiKKblCo53S"
      },
      "source": [
        "## 3.1: Baseline model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert 'DateOfBirth' Feature to Age\n",
        "data=df3\n",
        "current_year = datetime.datetime.now().year\n",
        "data['Age'] = data['DateOfBirth'].apply(lambda dob: current_year - pd.to_datetime(dob).year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropping features not helpful for the baseline model\n",
        "features_drppped = df3.drop(columns=['UserID', 'FirstName', 'LastName', 'Street', 'LicensePlate', 'DateOfBirth'])\n",
        "data = data.drop(features_drppped, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoding categorical features\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "data[categorical_cols] = data[categorical_cols].apply(label_encoder.fit_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>FirstName</th>\n",
              "      <th>LastName</th>\n",
              "      <th>Street</th>\n",
              "      <th>LicensePlate</th>\n",
              "      <th>DateOfBirth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>218-84-8180</td>\n",
              "      <td>Debra</td>\n",
              "      <td>Schaefer</td>\n",
              "      <td>503 Linda Locks</td>\n",
              "      <td>395C</td>\n",
              "      <td>1964-04-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>395-49-9764</td>\n",
              "      <td>Derek</td>\n",
              "      <td>Wright</td>\n",
              "      <td>969 Cox Dam Suite 101</td>\n",
              "      <td>UFZ 691</td>\n",
              "      <td>1978-06-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>892-81-4890</td>\n",
              "      <td>Shannon</td>\n",
              "      <td>Smith</td>\n",
              "      <td>845 Kelly Estate</td>\n",
              "      <td>48A•281</td>\n",
              "      <td>1972-03-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>081-11-7963</td>\n",
              "      <td>Christina</td>\n",
              "      <td>Brooks</td>\n",
              "      <td>809 Burns Creek</td>\n",
              "      <td>30Z J39</td>\n",
              "      <td>1985-02-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>347-03-9639</td>\n",
              "      <td>Ralph</td>\n",
              "      <td>Anderson</td>\n",
              "      <td>248 Brandt Plains Apt. 465</td>\n",
              "      <td>71-Q331</td>\n",
              "      <td>1983-08-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        UserID  FirstName  LastName                      Street LicensePlate  \\\n",
              "0  218-84-8180      Debra  Schaefer             503 Linda Locks         395C   \n",
              "1  395-49-9764      Derek    Wright       969 Cox Dam Suite 101      UFZ 691   \n",
              "2  892-81-4890    Shannon     Smith            845 Kelly Estate      48A•281   \n",
              "3  081-11-7963  Christina    Brooks             809 Burns Creek      30Z J39   \n",
              "4  347-03-9639      Ralph  Anderson  248 Brandt Plains Apt. 465      71-Q331   \n",
              "\n",
              "  DateOfBirth  \n",
              "0  1964-04-07  \n",
              "1  1978-06-02  \n",
              "2  1972-03-18  \n",
              "3  1985-02-26  \n",
              "4  1983-08-08  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data.copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Scores for each fold: [0.40963855 0.45592705 0.44585987 0.45859873 0.37540453]\n",
            "Mean F1 Score: 0.42908574707181035\n"
          ]
        }
      ],
      "source": [
        "# Defining the model (using default parameters)\n",
        "model = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Perform 5-fold cross-validation and use F1 score as the performance metric\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring=make_scorer(f1_score))\n",
        "\n",
        "# Print the results\n",
        "print(\"F1 Scores for each fold:\", scores)\n",
        "print(\"Mean F1 Score:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugyTS51Ko5vz"
      },
      "source": [
        "## 3.2: Adding feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'NumberPets'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NumberPets'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#1 Pets/married interaction feature\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPets_Married_Interaction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNumberPets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarried\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPets_Married_Interaction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumberPets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarried\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#2 Stability Index as a weighted sum of several features\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NumberPets'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#1 Pets/married interaction feature\n",
        "X_train['Pets_Married_Interaction'] = X_train['NumberPets'] * X_train['Married']\n",
        "X_test['Pets_Married_Interaction'] = X_test['NumberPets'] * X_test['Married']\n",
        "\n",
        "#2 Stability Index as a weighted sum of several features\n",
        "X_train['Stability_Index'] = (X_train['ResidenceDuration'] * 0.5) + \\\n",
        "                             (X_train['Married'] * 0.3) + \\\n",
        "                             (X_train['PreviousAccounts'] * 0.2)\n",
        "X_test['Stability_Index'] = (X_test['ResidenceDuration'] * 0.5) + \\\n",
        "                            (X_test['Married'] * 0.3) + \\\n",
        "                            (X_test['PreviousAccounts'] * 0.2)\n",
        "\n",
        "\n",
        "#3 Standard Scaling on numerical features\n",
        "numerical_cols = data.select_dtypes(include=['int64']).columns\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Define the PolynomialFeatures transformer\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "#4 Polynomial Features: Creating interaction terms (degree=2)\n",
        "poly_features = poly.fit_transform(data[numerical_cols])\n",
        "data = data.drop(numerical_cols, axis=1)\n",
        "data = pd.concat([data, pd.DataFrame(poly_features, columns=poly.get_feature_names_out(numerical_cols))], axis=1)\n",
        "\n",
        "# Define the PCA transformer\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "#5 PCA: Dimensionality reduction\n",
        "pca_features = pca.fit_transform(data)\n",
        "data = pd.concat([data, pd.DataFrame(pca_features, columns=['PCA1', 'PCA2'])], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retraining the model with modified data\n",
        "scores_with_feature_engineering = cross_val_score(model, X, y, cv=5, scoring=make_scorer(f1_score))\n",
        "\n",
        "# Calculating the mean score\n",
        "mean_score_with_feature_engineering = scores_with_feature_engineering.mean()\n",
        "mean_score_with_feature_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After feature engineering, the random classifer model achieved a mean F1 score of approximately 0.348 across 5-fold cross-validation. This score is slightly higher than the original baseline model's score of 0.327, indicating a minor improvement in the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Is 'NumberPets' in X_train?\", 'NumberPets' in X_train.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsdD0clko5pz"
      },
      "source": [
        "## 3.3: Adding feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Retraining the RandomForest model with the modified dataset\n",
        "model = RandomForestClassifier(random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Getting feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Creating a SelectFromModel object with a threshold or a specific number of features\n",
        "# For example, selecting features with importance higher than the mean importance\n",
        "selector = SelectFromModel(model, threshold='mean')\n",
        "\n",
        "# Fitting the selector and transforming the training and test sets\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Retraining the model with only the selected features\n",
        "model_selected = RandomForestClassifier(random_state=0)\n",
        "scores_with_selected_features = cross_val_score(model_selected, X_train_selected, y_train, cv=5, scoring=make_scorer(f1_score))\n",
        "\n",
        "# Calculating the mean score with the selected features\n",
        "mean_score_with_selected_features = scores_with_selected_features.mean()\n",
        "\n",
        "# Optionally, you can get the names of the selected features for analysis\n",
        "selected_features = X_train.columns[selector.get_support()]\n",
        "\n",
        "print(mean_score_with_selected_features)\n",
        "print(selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "F1 = 0.3243932707747182\n",
        "\n",
        "this score is nearly identical to the original baseline model's performance (F1 = 0.3268).\n",
        "\n",
        "This result suggests that the feature selection process, in this case, did not significantly alter the model's ability to predict credit risk. It's important to note that the effectiveness of feature selection can vary depending on the dataset and the model used. In some cases, it might lead to improvements, while in others, it may not have a substantial impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff4l2aNKo5fr"
      },
      "source": [
        "## 3.4: Adding hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 20],\n",
        "    'min_samples_split': [2, 4],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object with RandomForestClassifier and the parameter grid\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=0), param_grid, cv=5, scoring=make_scorer(f1_score), n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_selected, y)\n",
        "\n",
        "# Best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "best_params, best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "max_depth: None (indicating no limit on the depth of the trees)\n",
        "min_samples_leaf: 1 (the minimum number of samples required to be at a leaf node)\n",
        "min_samples_split: 4 (the minimum number of samples required to split an internal node)\n",
        "n_estimators: 200 (the number of trees in the forest)\n",
        "With these parameters, the model achieved a mean F1 score of approximately 0.343, which is an improvement over the initial baseline model score of 0.327. This demonstrates the effectiveness of hyperparameter tuning in enhancing the performance of machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te9gGGLEpXRG"
      },
      "source": [
        "## 3.5: Performance estimation on testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Training the model with the best parameters on the training data\n",
        "optimized_model = RandomForestClassifier(\n",
        "    n_estimators=200, \n",
        "    max_depth=None, \n",
        "    min_samples_split=4, \n",
        "    min_samples_leaf=1, \n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "optimized_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the testing data\n",
        "y_pred = optimized_model.predict(X_test)\n",
        "\n",
        "# Calculating the F1 score on the testing data\n",
        "test_f1_score = f1_score(y_test, y_pred)\n",
        "test_f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimized RandomForestClassifier, when evaluated on the testing data (representing \"future, unseen data\"), achieved an F1 score of approximately 0.372. This score indicates of how well the model might perform in a production environment.\n",
        "\n",
        "An F1 score of 0.372 suggests that the model has a reasonable balance between precision and recall, which are crucial for binary classification tasks like predicting credit risk. This score is slightly higher than the score obtained during the cross-validation phase (0.343), indicating that the model generalizes well to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPiErnUaTQSk"
      },
      "source": [
        "# Question 4: Uncle Steve's Wind Farm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug7A_K0yj7MR"
      },
      "source": [
        "**Current Situtation**\n",
        "- 256 Failed Turbines\n",
        "- Failure Repair Cost: $20,000 per turbine\n",
        "- Mainteance Service Cost: $2,000 per turbine\n",
        "- Inspection CostL $500 per turbine\n",
        "\n",
        "Uncle Steve is currently paying $5.12 million in maintenance costs without any predictive mainteance models,\n",
        "\n",
        "- number of fails * failure repair cost per turbine = 256 turbines * $20,000 = $5,120,000\n",
        "\n",
        "Random Forest Model will save $3,492,500 for Uncle Steve and cost less than RNN Model.\n",
        "\n",
        "Additional metrics like\n",
        "\n",
        "\n",
        "|         | Cost           |Savings   |\n",
        "| ------------- |:------------:|:------------:\n",
        "| No Predictive Models| $5,120,000 |  -    |\n",
        "| **Random Forest** | $1,627,500 | $3,492,500 |\n",
        "| Recurrent Neural Network| $1,765,000 | $3,355,000 |\n",
        "\n",
        "\n",
        "**Random Forest Cost Analysis**\n",
        "\n",
        "Confusion matrix for the random forest:\n",
        "\n",
        "|         | Predicted Fail           | Predicted No Fail  |Subtotal |\n",
        "| ------------- |:------------:| :-----:|:-----:|\n",
        "| **Actual Fail**      | 201 | 55 |256|\n",
        "| **Actual No Fail**   | 50 | 255,195 |255,245|\n",
        "| Subtotal                    | 251|255,250|255,501|\n",
        "\n",
        "Cost matrix for the random forest:\n",
        "\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      | $2500 | $20,000 |\n",
        "| **Actual No Fail**   | $500 | - |\n",
        "\n",
        "Total Cost for the random forest:\n",
        "\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      | $502,500 | $1,100,000 |\n",
        "| **Actual No Fail**   | $25,000 | - |\n",
        "\n",
        "Total Cost for Random Forest = $502,500 + $25,000 + $1,100,000 =  $1,627,500\n",
        "\n",
        "**RNN Cost Analysis**\n",
        "\n",
        "Confusion matrix for the RNN:\n",
        "\n",
        "|         | Predicted Fail           | Predicted No Fail  |Subtotal |\n",
        "| ------------- |:------------:| :-----:|:-----:|\n",
        "| **Actual Fail**      | 226 | 30 |256|\n",
        "| **Actual No Fail**   | 1200 | 25,4045 | 254,245|\n",
        "| Subtotal                   | 1426 | 254,075| 255,501|\n",
        "\n",
        "Cost matrix for the RNN:\n",
        "\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      | $2500 | $20,000 |\n",
        "| **Actual No Fail**   | $500 | - |\n",
        "\n",
        "Total Cost for the RNN:\n",
        "\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      |  $565,000  | $600,000  |\n",
        "| **Actual No Fail**   |  $600,000  | - |\n",
        "\n",
        "Total Cost for RNN = $565,000 + $600,000 + $600,000 = $1,765,000"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
