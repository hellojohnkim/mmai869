{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellojohnkim/mmai869/blob/main/2024_869_JohnKim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MMAI 869 2024: Individual Assignment\n",
        "\n",
        "- Student Name: Kim, John\n",
        "- Student Number: 20439250\n",
        "- Section Number: MMAI 2024\n",
        "- Favourite Book: Pachinko by Min Jin Lee\n",
        "- Currently Reading: The Worlds I See by Fei Fei Li\n",
        "- Submitted Date: 2024-01-07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj34Jz-Do_oK"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQ_XOKyXTS6",
        "outputId": "25fe5c2c-939f-49eb-aaf6-0f3caffc998c"
      },
      "outputs": [],
      "source": [
        "print(datetime.datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfOMt1lErLhZ",
        "outputId": "a667defb-07cb-493a-94b1-5d906420cc07"
      },
      "outputs": [],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aub2w1-arM5K",
        "outputId": "27c8b598-d2cc-4e89-b5d0-204d57ead224"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Y_n_8UrO9i",
        "outputId": "4a5563d9-1834-429b-ce97-fbd49a00ea5f"
      },
      "outputs": [],
      "source": [
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLlBjIyS2o54"
      },
      "source": [
        "# Question 1: Uncle Steve's Diamonds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj1NSQelo_oN"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "You work at a local jewelry store named *Uncle Steve's Diamonds*. You started as a janitor, but you’ve recently been promoted to senior data analyst. Congratulations!\n",
        "\n",
        "Uncle Steve, the store's owner, needs to better understand the store's customers. In particular, he wants to know what kind of customers shop at the store. He wants to know the main types of *customer personas*. Once he knows these, he will contemplate ways to better market to each persona, better satisfy each persona, better cater to each persona, increase the loyalty of each persona, etc. But first, he must know the personas.\n",
        "\n",
        "You want to help Uncle Steve. Using sneaky magic (and the help of Environics), you've collected four useful features for a subset of the customers: age, income, spending score (i.e., a score based on how much they’ve spent at the store in total), and savings (i.e., how much money they have in their bank account).\n",
        "\n",
        "**Your tasks**\n",
        "\n",
        "1. Pick a clustering algorithm (the [`sklearn.cluster`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster) module has many good choices, including [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans), [`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN), and [`AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) (aka Hierarchical)). (Note that another popular implementation of the hierarchical algorithm can be found in SciPy's [`scipy.cluster.hierarchy.linkage`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html).) Don't spend a lot of time thinking about which algorithm to choose - just pick one. Cluster the customers as best as you can, within reason. That is, try different feature preprocessing steps, hyperparameter values, and/or distance metrics. You don't need to try every posssible combination, but try a few at least. Measure how good each  model configuration is by calculating an internal validation metric (e.g., [`calinski_harabasz_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html) or [`silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)).\n",
        "2. You have some doubts - you're not sure if the algorithm you chose in part 1 is the best algorithm for this dataset/problem. Neither is Uncle Steve. So, choose a different algorithm (any!) and do it all again.\n",
        "3. Which clustering algorithm is \"better\" in this case? Think about charateristics of the algorithm like quality of results, ease of use, speed, interpretability, etc. Choose a \"winner\" and justify to Uncle Steve.\n",
        "4. Interpret the clusters of the winning model. That is, describe, in words, a *persona* that accurately depicts each cluster. Use statistics (e.g., cluster means/distributions), examples (e.g., exemplar instances from each cluster), and/or visualizations (e.g., relative importance plots, snakeplots) to get started. Human judgement and creativity will be necessary. This is where it all comes together. Be descriptive and *help Uncle Steve understand his customers better*. Please!\n",
        "\n",
        "**Marking**\n",
        "\n",
        "The coding parts (i.e., 1 and 2) will be marked based on:\n",
        "\n",
        "- *Correctness*. Code clearly and fully performs the task specified.\n",
        "- *Reproducibility*. Code is fully reproducible. I.e., you (and I) are able to run this Notebook again and again, from top to bottom, and get the same results each time.\n",
        "- *Style*. Code is organized. All parts commented with clear reasoning and rationale. No old code laying around. Code easy to follow.\n",
        "\n",
        "\n",
        "Parts 3 and 4 will be marked on:\n",
        "\n",
        "- *Quality*. Response is well-justified and convincing. Responses uses facts and data where possible.\n",
        "- *Style*. Response uses proper grammar, spelling, and punctuation. Response is clear and professional. Response is complete, but not overly-verbose. Response follows length guidelines.\n",
        "\n",
        "\n",
        "**Tips**\n",
        "\n",
        "- Since clustering is an unsupervised ML technique, you don't need to split the data into training/validation/test or anything like that. Phew!\n",
        "- On the flip side, since clustering is unsupervised, you will never know the \"true\" clusters, and so you will never know if a given algorithm is \"correct.\" There really is no notion of \"correctness\" - only \"usefullness.\"\n",
        "- Many online clustering tutorials (including some from Uncle Steve) create flashy visualizations of the clusters by plotting the instances on a 2-D graph and coloring each point by the cluster ID. This is really nice and all, but it can only work if your dataset only has exactly two features - no more, no less. This dataset has more than two features, so you cannot use this technique. (But that's OK - you don't need to use this technique.)\n",
        "- Must you use all four features in the clustering? Not necessarily, no. But \"throwing away\" quality data, for no reason, is unlikely to improve a model.\n",
        "- Some people have success applying a dimensionality reduction technique (like [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) to the features before clustering. You may do this if you wish, although it may not be as helpful in this case because there are only four features to begin with.\n",
        "- If you apply a transformation (e.g., [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) or [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)) to the features before clustering, you may have difficulty interpretting the means of the clusters (e.g., what is a mean Age of 0.2234??). There are two options to fix this: first, you can always reverse a transformation with the `inverse_transform` method. Second, you can just use the original dataset (i.e., before any preprocessing) during the interpreation step.\n",
        "- You cannot change the distance metric for K-Means. (This is for theoretical reasons: K-Means only works/makes sense with Euclidean distance.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yww0-vtpOw7z"
      },
      "source": [
        "## 1.0: Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVWx2c-DhQYo",
        "outputId": "c63c2c7d-17e8-437c-a81e-543d28a45c0a"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "df1 = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1thHDCwQK3GijytoSSZNekAsItN_FGHtm\")\n",
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "zGojixXVCVNY",
        "outputId": "743471ec-66a2-4c2c-be36-f9ca5705d330"
      },
      "outputs": [],
      "source": [
        "## Data Exploration\n",
        "df1.describe().transpose()\n",
        "list(df1)\n",
        "df1.shape\n",
        "df1.info()\n",
        "df1.head(n=20)\n",
        "df1.describe().transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df1.copy()\n",
        "X.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names = df1.columns\n",
        "X = df1.to_numpy()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-processing steps before clustering \n",
        "# use standard scaler to standardize the values in the data before we apply clsutering\n",
        "scaler = StandardScaler()\n",
        "features = df1.columns\n",
        "X[features] = scaler.fit_transform(X[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape\n",
        "X.info()\n",
        "X.describe().transpose()\n",
        "X.head(10)\n",
        "X.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure();\n",
        "\n",
        "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=\"black\");\n",
        "plt.title(\"Jewelry Customers Data\");\n",
        "plt.xlabel('Annual Income (K)');\n",
        "plt.ylabel('Spending Score');\n",
        "plt.xticks();\n",
        "plt.yticks();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R04NzckZKbG2"
      },
      "source": [
        "## 1.1: Clustering Algorithm #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KMeans(random_state=42)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_means = KMeans(init=\"k-means++\", random_state=42)\n",
        "k_means.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([6, 6, 1, 1, 0, 3, 1, 0, 5, 0, 4, 2, 7, 7, 1, 4, 1, 5, 0, 4, 7, 4,\n",
              "       6, 5, 4, 3, 3, 1, 0, 1, 1, 1, 0, 1, 2, 6, 0, 1, 2, 7, 1, 0, 3, 6,\n",
              "       1, 3, 4, 3, 2, 6, 0, 2, 3, 5, 1, 0, 2, 0, 5, 5, 5, 0, 0, 6, 1, 6,\n",
              "       0, 3, 3, 1, 1, 7, 5, 0, 7, 6, 2, 6, 1, 3, 1, 4, 6, 5, 5, 2, 7, 6,\n",
              "       1, 6, 1, 6, 3, 0, 6, 3, 2, 6, 6, 2, 7, 3, 2, 3, 2, 0, 5, 7, 7, 6,\n",
              "       5, 2, 6, 2, 7, 7, 6, 7, 3, 0, 2, 0, 5, 5, 2, 4, 1, 0, 6, 1, 2, 1,\n",
              "       7, 6, 3, 0, 4, 3, 1, 2, 0, 6, 1, 3, 1, 3, 6, 4, 1, 5, 6, 7, 5, 6,\n",
              "       5, 4, 7, 6, 0, 1, 2, 5, 6, 7, 0, 2, 5, 6, 2, 6, 0, 1, 7, 1, 0, 6,\n",
              "       6, 0, 0, 0, 6, 6, 7, 3, 3, 5, 2, 1, 1, 6, 2, 0, 6, 1, 1, 0, 3, 7,\n",
              "       5, 5, 0, 2, 6, 2, 6, 2, 1, 0, 3, 1, 0, 0, 5, 5, 1, 5, 7, 3, 7, 0,\n",
              "       6, 6, 2, 0, 7, 3, 7, 6, 1, 0, 6, 6, 0, 6, 6, 6, 2, 1, 7, 2, 3, 1,\n",
              "       6, 6, 4, 6, 2, 0, 3, 3, 3, 5, 6, 7, 0, 0, 7, 7, 6, 6, 4, 1, 6, 0,\n",
              "       6, 5, 7, 3, 6, 7, 5, 1, 5, 0, 7, 6, 0, 5, 6, 6, 0, 2, 7, 4, 7, 5,\n",
              "       0, 7, 4, 0, 5, 3, 6, 7, 2, 6, 2, 0, 5, 2, 1, 7, 5, 1, 2, 3, 1, 5,\n",
              "       2, 2, 5, 4, 4, 1, 0, 7, 1, 6, 5, 7, 5, 2, 0, 7, 0, 0, 2, 5, 7, 6,\n",
              "       7, 1, 6, 2, 7, 5, 0, 3, 3, 7, 0, 3, 0, 4, 6, 6, 1, 0, 0, 7, 7, 4,\n",
              "       2, 4, 6, 7, 3, 5, 1, 0, 3, 6, 5, 5, 1, 0, 2, 6, 4, 7, 2, 1, 0, 5,\n",
              "       0, 0, 5, 2, 7, 5, 1, 2, 6, 4, 0, 5, 6, 5, 5, 6, 5, 0, 2, 5, 6, 2,\n",
              "       3, 7, 6, 5, 7, 1, 6, 1, 5, 2, 3, 7, 6, 2, 1, 6, 3, 2, 4, 3, 7, 2,\n",
              "       5, 1, 0, 2, 5, 7, 6, 6, 0, 4, 2, 0, 2, 1, 1, 7, 6, 5, 1, 2, 6, 5,\n",
              "       1, 0, 3, 2, 2, 3, 7, 2, 0, 5, 6, 3, 1, 7, 7, 4, 3, 7, 3, 0, 2, 7,\n",
              "       6, 2, 0, 0, 3, 6, 7, 1, 1, 2, 6, 1, 6, 5, 7, 7, 6, 6, 1, 5, 2, 5,\n",
              "       0, 5, 0, 3, 5, 5, 5, 1, 1, 0, 0, 6, 3, 5, 2, 3, 2, 0, 5, 0, 5],\n",
              "      dtype=int32)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the cluster labels for each data point\n",
        "labels = k_means.labels_\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.20746731, -1.33611726, -0.64538103,  1.14264671],\n",
              "       [ 0.00639037, -0.11090221,  1.04534171, -0.824199  ],\n",
              "       [-1.08606468,  0.77829813, -0.76260864,  0.79448196],\n",
              "       [-1.44466803,  1.46050665,  1.51057952, -1.57249636],\n",
              "       [ 1.11876151,  1.23565419, -1.68367042,  0.59588052],\n",
              "       [ 1.1748626 , -1.31231335, -0.71943742,  0.76977876],\n",
              "       [ 0.06420515, -0.0651299 ,  1.01300305, -1.14800405],\n",
              "       [-1.09030742,  0.87816916, -0.74187521,  0.4542097 ]])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#centroids values\n",
        "k_means.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[8.81392405e+01, 2.74701646e+04, 3.37686022e-01, 1.75120744e+04],\n",
              "       [5.91739130e+01, 7.15255507e+04, 7.76219861e-01, 7.78734682e+03],\n",
              "       [3.28281250e+01, 1.03498766e+05, 3.07279927e-01, 1.57906341e+04],\n",
              "       [2.41800000e+01, 1.28029120e+05, 8.96891640e-01, 4.08752031e+03],\n",
              "       [8.60000000e+01, 1.19944040e+05, 6.83780993e-02, 1.48086838e+04],\n",
              "       [8.73529412e+01, 2.83260882e+04, 3.18477530e-01, 1.56684935e+04],\n",
              "       [6.05681818e+01, 7.31713977e+04, 7.67831971e-01, 6.18634890e+03],\n",
              "       [3.27258065e+01, 1.07089855e+05, 3.12657694e-01, 1.41082170e+04]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler.inverse_transform(k_means.cluster_centers_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidIndexError",
          "evalue": "(slice(None, None, None), 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the data points with different colors for each cluster\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, X[:, \u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3660\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:5737\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5735\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5736\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
          ]
        }
      ],
      "source": [
        "# Plot the data points with different colors for each cluster\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the silhouette score for the KMeans model\n",
        "silhouette_score(X, labels)\n",
        "\n",
        "# Calculate the silhouette samples for the KMeans model\n",
        "silhouette_samples(X, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "import numpy as np\n",
        "\n",
        "# Preprocessing: Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df1)\n",
        "\n",
        "# Function to apply K-Means and evaluate the model\n",
        "def kmeans_clustering(data, n_clusters):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(data)\n",
        "\n",
        "    silhouette = silhouette_score(data, labels)\n",
        "    calinski_harabasz = calinski_harabasz_score(data, labels)\n",
        "\n",
        "    return kmeans, silhouette, calinski_harabasz\n",
        "\n",
        "# Trying K-Means with different numbers of clusters\n",
        "k_values = range(2, 11)\n",
        "kmeans_results = {}\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans, silhouette, calinski_harabasz = kmeans_clustering(scaled_data, k)\n",
        "    kmeans_results[k] = (silhouette, calinski_harabasz)\n",
        "\n",
        "kmeans_results_df = pd.DataFrame(kmeans_results, index=[\"Silhouette Score\", \"Calinski-Harabasz Score\"]).T\n",
        "kmeans_results_df.index.name = 'Number of Clusters'\n",
        "kmeans_results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihVtYBWg1NM6"
      },
      "source": [
        "## 1.2: Clustering Algorithm #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "#call the linkage function\n",
        "aggl = linkage(X, method='ward', metric='euclidean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "# Plot the dendogram 0 this time with better labels\n",
        "plt.figure(figsize=(16, 8));\n",
        "plt.grid(False)\n",
        "plt.title(\"Uncle Steve Diamonds Dendogram\");  \n",
        "dend = scipy.cluster.hierarchy.dendrogram(aggl); "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's find K=5 clusters\n",
        "K=5\n",
        "labels = scipy.cluster.hierarchy.fcluster(aggl, K, criterion=\"maxclust\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1['Cluster ID'] = labels\n",
        "df1.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ySJIgNr1Sfy"
      },
      "source": [
        "## 1.3 Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2EAnCJ1Xta"
      },
      "source": [
        "## 1.4 Personas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVewu2TZ1XhK"
      },
      "source": [
        "TODO: Delete this text and insert your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYwuYIgczYSv"
      },
      "source": [
        "# Question 2: Uncle Steve's Fine Foods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YQsOb6CzYVq"
      },
      "source": [
        "## 2.1: A rule that might have high support and high confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzXu1IvK-MEg"
      },
      "source": [
        "Rule: {organic vegetables} -> {plant-based milk}\n",
        "</p>\n",
        "The pairing of organic vegetables and plant-based milk reflects a growing consumer trend towards health-conscious and environmentally sustainable choices. This rule's presence in a large number of transactions is indicative of a substantial customer base that prioritizes organic and plant-based products. For Uncle Steve, this insight is valuable for inventory management and marketing strategies. It underscores the importance of stocking a diverse range of organic vegetables and plant-based milk options to cater to this health-focused demographic. Additionally, it presents an opportunity for Uncle Steve to position his store as a destination for customers seeking healthier, eco-friendly food options. This could involve creating dedicated sections for organic and plant-based products, running promotional health-focused campaigns, and possibly hosting in-store events or workshops centered around healthy living and sustainability. Such initiatives could enhance customer loyalty and attract new customers who are increasingly making purchasing decisions based on health and environmental considerations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNIrAgJk-L4l"
      },
      "source": [
        "## 2.2: A rule that might have reasonably high support but low confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svdzYW3S-LvF"
      },
      "source": [
        "Rule: {energy drinks} -> {gaming accessories}\n",
        "</p>\n",
        "The association between energy drinks and gaming accessories, while not always consistent in every transaction, points to a specific lifestyle or customer hobby. Energy drinks are popular among gamers for their stimulating effects during long gaming sessions, but the purchase of gaming accessories is a less frequent occurrence. This rule's insight for Uncle Steve lies in the potential to tap into the gaming community by cross-promoting these products. Although the confidence in the rule is low, indicating that not all buyers of energy drinks are interested in gaming accessories, there is still an opportunity to increase sales through targeted marketing efforts. Uncle Steve could consider setting up a gaming section in his store, featuring energy drinks alongside gaming accessories, or even collaborate with local gaming events for mutual promotion. This approach not only caters to the existing customer base but also positions the store as a hub for the local gaming community, potentially attracting a younger demographic and creating a unique shopping experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loe--LMz-Ll8"
      },
      "source": [
        "## 2.3: A rule that might have low support and low confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdyOB5fe-Zgy"
      },
      "source": [
        "Rule: {gourmet mustard} -> {imported chocolates}\n",
        "</p>\n",
        "The combination of gourmet mustard and imported chocolates is unconventional, reflecting a unique, perhaps adventurous, consumer taste. Given their niche nature, these items are unlikely to be frequently purchased together, leading to low support and confidence in this rule. For Uncle Steve, this unusual pairing may not be particularly actionable due to its rarity. However, it can serve as a creative prompt to explore less obvious product combinations that might appeal to a small but potentially loyal customer segment. This insight could encourage experimenting with diverse, high-end products to attract customers seeking unique culinary experiences. While not a primary strategy, it could add an element of surprise and novelty to the store's product range, potentially distinguishing Uncle Steve's store from more conventional grocery outlets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St2eI3We-ZYs"
      },
      "source": [
        "## 2.4: A rule that might have low support and high confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcdZc-B1-fE7"
      },
      "source": [
        "Rule: {camping gear} -> {trail mix}\n",
        "</p>\n",
        "The connection between camping gear and trail mix, while occurring in a smaller number of transactions, shows a high likelihood of joint purchase when camping gear is bought. This pattern reflects a specific customer interest in outdoor activities. For Uncle Steve, this rule highlights an opportunity to cater to the outdoor enthusiast segment. Although the frequency of such purchases may be low (low support), the strong association (high confidence) suggests that those who are buying camping gear are very likely to be interested in trail mix as well. This insight could lead Uncle Steve to strategically position and market these items together, possibly creating an outdoor-themed section in his store. Additionally, it opens avenues for seasonal promotions, especially during peak camping seasons, and partnerships with local outdoor activity groups or events. Focusing on this niche could not only increase sales in these categories but also enhance the store's reputation as a community-focused retailer that understands and caters to the specific interests and needs of its customers. Such targeted efforts can create a loyal customer base and differentiate Uncle Steve's store from larger, more generic retailers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_IHoz7f2yIV"
      },
      "source": [
        "# Question 3: Uncle Steve's Credit Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqm_REd4oouz"
      },
      "source": [
        "## 3.0: Load data and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6b_BM0Nz9sF",
        "outputId": "0ea6da79-f51b-4f05-e2e1-f0eea595449c"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# First, we'll read the provided labeled training data\n",
        "df3 = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1wOhyCnvGeY4jplxI8lZ-bbYN3zLtickf\")\n",
        "df3.info()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df3.drop('BadCredit', axis=1) #.select_dtypes(['number'])\n",
        "y = df3['BadCredit']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Q-h0aAKKMn",
        "outputId": "0d0cbbfb-2c84-403c-abae-a08dc7e78a32"
      },
      "outputs": [],
      "source": [
        "X.head()\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "X_train.select_dtypes(include=['object']).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdiKKblCo53S"
      },
      "source": [
        "## 3.1: Baseline model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCkLW1Ekj7MK",
        "outputId": "f5b2022c-1460-495a-c521-85a50bed02ee"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Identifying categorical columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Creating a column transformer for one-hot encoding categorical variables\n",
        "preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)], remainder='passthrough')\n",
        "\n",
        "# Creating a pipeline with preprocessor and a Random Forestclassifier\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "# Performing 10-fold cross-validation to evaluate the baseline model\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='roc_auc')\n",
        "\n",
        "# Displaying the mean score\n",
        "print(\"Mean ROC AUC Score:\", scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugyTS51Ko5vz"
      },
      "source": [
        "## 3.2: Adding feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsdD0clko5pz"
      },
      "source": [
        "## 3.3: Adding feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff4l2aNKo5fr"
      },
      "source": [
        "## 3.4: Adding hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te9gGGLEpXRG"
      },
      "source": [
        "## 3.5: Performance estimation on testing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPiErnUaTQSk"
      },
      "source": [
        "# Question 4: Uncle Steve's Wind Farm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug7A_K0yj7MR"
      },
      "source": [
        "# Answers\n",
        "**Current Situtation**\n",
        "- 256 Failed Turbines\n",
        "- Failure Repair Cost: $20,000 per turbine\n",
        "- Mainteance Service Cost: $2,000 per turbine\n",
        "- Inspection CostL $500 per turbine\n",
        "\n",
        "Uncle Steve is currently paying $5.12 million in maintenance costs without any predictive mainteance models,\n",
        "\n",
        "- number of fails * failure repair cost per turbine = 256 turbines * $20,000 = $5,120,000\n",
        "\n",
        "Random Forest Model will save $3,492,500 for Uncle Steve and cost less than RNN Model.\n",
        "\n",
        "Additional metrics like\n",
        "\n",
        "|         | Cost           |Savings   |\n",
        "| ------------- |:------------:|:------------:\n",
        "| **No Predictive Models**      | $5,120,000 |  -    |\n",
        "| **Random Forest**   | **$1,627,500** | **$3,492,500** |\n",
        "| **Recurrent Neural Network**   | $1,765,000 | $3,355,000 |\n",
        "\n",
        "\n",
        "**Random Forest Cost Analysis**\n",
        "\n",
        "Confusion matrix for the random forest:\n",
        "|         | Predicted Fail           | Predicted No Fail  | |\n",
        "| ------------- |:------------:| :-----:|:-----:|\n",
        "| **Actual Fail**      | 201 | 55 |256|\n",
        "| **Actual No Fail**   | 50 | 255,195 |255,245|\n",
        "|                      | 251|255,250|255,501|\n",
        "\n",
        "Cost matrix for the random forest:\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      | $2500 | $20,000 |\n",
        "| **Actual No Fail**   | $500 | - |\n",
        "\n",
        "Total Cost for the random forest:\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      | $502,500 | $1,100,000 |\n",
        "| **Actual No Fail**   | $25,000 | - |\n",
        "\n",
        "Total Cost = $502,500 + $25,000 + $1,100,000 =  $1,627,500\n",
        "\n",
        "**RNN Cost Analysis**\n",
        "\n",
        "Confusion matrix for the RNN:\n",
        "|         | Predicted Fail           | Predicted No Fail  | |\n",
        "| ------------- |:------------:| :-----:|:-----:|\n",
        "| **Actual Fail**      | 226 | 30 |256|\n",
        "| **Actual No Fail**   | 1200 | 25,4045 | 254,245|\n",
        "|                      | 1426 | 254,075| 255,501|\n",
        "\n",
        "Cost matrix for the RNN:\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      | $2500 | $20,000 |\n",
        "| **Actual No Fail**   | $500 | - |\n",
        "\n",
        "Total Cost for the RNN:\n",
        "|         | Predicted Fail           | Predicted No Fail  |\n",
        "| ------------- |:------------:| :-----:|\n",
        "| **Actual Fail**      |  $565,000  | $600,000  |\n",
        "| **Actual No Fail**   |  $600,000  | - |\n",
        "\n",
        "Total Cost = $565,000 + $600,000 + $600,000 = $1,765,000"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
